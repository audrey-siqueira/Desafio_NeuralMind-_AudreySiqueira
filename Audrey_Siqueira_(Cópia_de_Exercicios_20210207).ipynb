{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audrey Siqueira (Cópia de Exercicios - 20210207)",
      "provenance": [],
      "collapsed_sections": [
        "mTVOQpMfhgLM",
        "udS0Ns4etoJs",
        "RcMz3Vzjt144",
        "2iI7udBFeDlP",
        "JtgSAAKjUfcO",
        "GF_P_GARU62m",
        "cbXIXsDIUmtp",
        "QKnLAyL7zgpa",
        "Flr1lI5o-HpG",
        "ZF_-dJ2nCZtT",
        "vsqzALS4CZtl",
        "qjEl-0l7CZt0",
        "eMwwVtJ1CZt2",
        "XC96wB7PCZt8",
        "LsOThnt8fDJV",
        "O_Sx1QXZxJ3u",
        "JBXxBmWGK3IU",
        "GulfYtzBMx2e",
        "WsrSF8GEiXk4",
        "35I5w8EZdjIo",
        "3UNdHqgSB6S9"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrey-siqueira/Desafio_NeuralMind-_AudreySiqueira/blob/main/Audrey_Siqueira_(C%C3%B3pia_de_Exercicios_20210207).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "## Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829c1855-1850-47cb-cdb5-d63fa2493d39"
      },
      "source": [
        "print('Meu nome é: Audrey Siqueira')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Meu nome é: Audrey Siqueira\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e/ou `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L, k):\n",
        "    a=dict(Counter(L).most_common(k))\n",
        "    print(a)\n",
        "    return"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685538f7-0642-4b18-e6ee-0f8558e94b0e"
      },
      "source": [
        "from collections import Counter\n",
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 4, 'd': 3, 'e': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00245cd7-25bb-478d-983f-3954d3c0a96c"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'k': 385544, 'y': 385506, 't': 385319, 'o': 385263, 'q': 385216, 'j': 384927, 'r': 384903, 'u': 384826, 'f': 384785, 'l': 384763, 'w': 384728, 'e': 384714, 'p': 384699, 'g': 384646, 's': 384630, 'c': 384599, 'h': 384570, 'i': 384346, 'v': 384279, 'z': 384240, 'n': 384114, 'm': 384048, 'a': 384011, 'b': 383871, 'd': 383822, 'x': 383631}\n",
            "{'k': 385544, 'y': 385506, 't': 385319, 'o': 385263, 'q': 385216, 'j': 384927, 'r': 384903, 'u': 384826, 'f': 384785, 'l': 384763, 'w': 384728, 'e': 384714, 'p': 384699, 'g': 384646, 's': 384630, 'c': 384599, 'h': 384570, 'i': 384346, 'v': 384279, 'z': 384240, 'n': 384114, 'm': 384048, 'a': 384011, 'b': 383871, 'd': 383822, 'x': 383631}\n",
            "{'k': 385544, 'y': 385506, 't': 385319, 'o': 385263, 'q': 385216, 'j': 384927, 'r': 384903, 'u': 384826, 'f': 384785, 'l': 384763, 'w': 384728, 'e': 384714, 'p': 384699, 'g': 384646, 's': 384630, 'c': 384599, 'h': 384570, 'i': 384346, 'v': 384279, 'z': 384240, 'n': 384114, 'm': 384048, 'a': 384011, 'b': 383871, 'd': 383822, 'x': 383631}\n",
            "{'k': 385544, 'y': 385506, 't': 385319, 'o': 385263, 'q': 385216, 'j': 384927, 'r': 384903, 'u': 384826, 'f': 384785, 'l': 384763, 'w': 384728, 'e': 384714, 'p': 384699, 'g': 384646, 's': 384630, 'c': 384599, 'h': 384570, 'i': 384346, 'v': 384279, 'z': 384240, 'n': 384114, 'm': 384048, 'a': 384011, 'b': 383871, 'd': 383822, 'x': 383631}\n",
            "1 loop, best of 3: 560 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text, vocabulary):\n",
        "     import re\n",
        "     import pandas as pd\n",
        "\n",
        "     d=re.findall(r\"[\\w']+|[.,!?;]\",D)\n",
        "     \n",
        "     fora=list(set(d)-set(V.keys()))\n",
        "     a={}\n",
        "     a=a.fromkeys(fora,V['unknown'])\n",
        "     V.update(a)\n",
        "     \n",
        "     #MÉTODO NATIVO SEM AJUDA DO PANDAS PORÉM NÃO ESCALONÁVEL\n",
        "     #Lista_Identificadores =list(map(V.get, d))\n",
        "     #print(Lista_Identificadores)\n",
        "\n",
        "     #ESCALONÁVEL COM PANDAS\n",
        "     Lista_Identificadores = pd.DataFrame().append(V,ignore_index=True)\n",
        "     print(Lista_Identificadores[d].values[0])\n",
        "\n",
        "     return"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e26edb-e1b0-40fa-fc8d-b6ce273a2fb6"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "tokens_to_ids(D, V)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.  3.  2.  4. -1.  5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000* ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccabe5d-f5a4-4fe6-ccdd-2aa94ad97d64"
      },
      "source": [
        "%%timeit\n",
        "tokens_to_ids(D, V)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.  3.  2. ...  4. -1.  5.]\n",
            "[-1.  3.  2. ...  4. -1.  5.]\n",
            "[-1.  3.  2. ...  4. -1.  5.]\n",
            "[-1.  3.  2. ...  4. -1.  5.]\n",
            "1 loop, best of 3: 3.93 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int):\n",
        "     import random\n",
        "     arquivo = open(filename)\n",
        "     arquivo_conteudo = arquivo.read()\n",
        "     arquivo_separado = arquivo_conteudo.splitlines()\n",
        "\n",
        "     random.shuffle(arquivo_separado)\n",
        "\n",
        "     return arquivo_separado[0:k]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3627b40d-4671-43b6-c776-712d20c4fce3"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['line 52', 'line 13', 'line 77', 'line 39', 'line 58', 'line 72', 'line 43', 'line 30', 'line 97', 'line 95']\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632e4743-5902-49e8-dbd0-594b654b2328"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 995 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: **mp(n −1)** \n",
        "- número de multiplicações: **mnp** \n",
        "- número de flops: **2mnp-mp (aproximadamente 2mnp )**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6bd6e2-7827-4903-ab82-6197d084069d"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844c5c25-5b9e-4331-d8c0-4c8f1f00a4b3"
      },
      "source": [
        "np.vstack(A.mean(1))\r\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.5],\n",
              "       [ 8.5],\n",
              "       [14.5],\n",
              "       [20.5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e08860a-8251-4733-9fd4-3574863ec414"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "num= A.min()\n",
        "den=(A.max()-A.min())\n",
        "Normalized=(A-num)/(den)\n",
        "print(Normalized)\n",
        "\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMPwRkr4v1vC",
        "outputId": "d74a5e4a-a421-4a9f-9f44-4ac797affd31"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\r\n",
        "print(A)\r\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a455ee9-267a-4345-9369-f675b2b81ece"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "\n",
        "mins = np.min(A, axis=0)\n",
        "maxs = np.max(A, axis=0)\n",
        "dens = maxs - mins\n",
        "\n",
        "Normalized_by_Columns = (A-mins)/(dens)\n",
        "\n",
        "print(Normalized_by_Columns.round(2))\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.   0.   0.   0.   0.  ]\n",
            " [0.33 0.33 0.33 0.33 0.33 0.33]\n",
            " [0.67 0.67 0.67 0.67 0.67 0.67]\n",
            " [1.   1.   1.   1.   1.   1.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas. A solução deve funcionar para qualquer tamanho de matriz.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cae084e-8f1f-4978-d18c-3c5807d156b5"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "A=np.transpose(A)\n",
        "\n",
        "mins = np.min(A, axis=0)\n",
        "maxs = np.max(A, axis=0)\n",
        "dens = maxs - mins\n",
        "\n",
        "Normalized_by_Lines = (A-mins)/(dens)\n",
        "Normalized_by_Lines =np.transpose(Normalized_by_Lines)\n",
        "\n",
        "\n",
        "print(Normalized_by_Lines.round(2))\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em aprendizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    col_max = np.amax(A,axis=1,keepdims = True)\n",
        "    A_exp = np.exp(A - col_max)\n",
        "    A_sum = np.sum(A_exp, axis = 1, keepdims = True)\n",
        "    res = A_exp / A_sum\n",
        "    \n",
        "    return res\n",
        "     "
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa0J2YVOD1UM",
        "outputId": "31c20132-d1f1-46c0-8fe0-85e77dd6ecd2"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\r\n",
        "              [-2,   0, 0.5]])\r\n",
        "softmax(A)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "Rode o código a seguir para verificar se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6311ea-381b-41a6-fd16-2bc8e69d772d"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181c42fb-d4ce-4a9e-fbff-c0c8540379af"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 291 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESw1Aw5f7ltp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9557e315-5053-43d6-d57d-536d778906fc"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "   hot=np.squeeze(np.eye(n_classes)[y.reshape(-1)])\n",
        "   print(hot)\n",
        "   return"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75456043-e1c7-4974-c0bd-835247be6029"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 7 4 0 3 0 8 8 8 8]\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9521d76a-0800-4689-b2bf-f213fabbb821"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "1 loop, best of 3: 162 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88bf80e3-c087-4d48-ace3-14ad1e23d420"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "## Tensor com atributo .requires_grad=*True*\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043ec861-2ee2-4a81-c3e6-6eb059d7c4c4"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5264b54b-b495-4424-f3b1-62eb604e5645"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd67c606-8793-4207-f8df-2c556b69b36b"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "### Montagem do grafo computacional - forward\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2b68f1-abb7-456c-cc47-a7a5e9906ed2"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Auto grad - processa o grafo computacional backward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15078637-da25-441d-9413-47c3db7b50c4"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-28.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e581cd-1bed-4b0c-ecf6-e742a993c9b9"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAmySxT5tbMc",
        "outputId": "ccf66baa-4fc1-47ca-992e-99b5e6f90d38"
      },
      "source": [
        "x = torch.arange(0, 4).float()\r\n",
        "print(x)\r\n",
        "y = torch.arange(0, 8, 2).float()\r\n",
        "print(y)\r\n",
        "w = torch.ones(1)\r\n",
        "print(w)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.])\n",
            "tensor([0., 2., 4., 6.])\n",
            "tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpyQ7kUioJGk",
        "outputId": "de6723b2-d5c3-422a-dd1f-1293fd5a8e22"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "learning_rate = 0.01\r\n",
        "iteracoes = 1\r\n",
        "\r\n",
        "it=[]\r\n",
        "Loss=[]\r\n",
        "\r\n",
        "def J_func(w, x, y):\r\n",
        "     \r\n",
        "    for i in range(iteracoes): \r\n",
        "\r\n",
        "      #Loss Function\r\n",
        "      y_pred = w*x \r\n",
        "      e = y_pred - y\r\n",
        "      e2 = e.pow(2) \r\n",
        "      J = e2.sum() \r\n",
        "\r\n",
        "      #Diferencas finitas EXERCÍCIO 1\r\n",
        "      deltaW=0.5\r\n",
        "      grad = (sum( ( (w+deltaW)*x - y)**2 ) - sum( ( (w-deltaW)*x - y)**2 ))/(2*deltaW)\r\n",
        "        \r\n",
        "      #Novo valor de w\r\n",
        "      w = w - learning_rate * grad\r\n",
        "\r\n",
        "      print(\"iteration {}\".format(i))\r\n",
        "      print (\"loss {} , gradient {} , weight {}\".format(J,grad,w))\r\n",
        "\r\n",
        "      #Para plotar o gráfico\r\n",
        "      it.append(i)\r\n",
        "      Loss.append(J)\r\n",
        "\r\n",
        "\r\n",
        "x = torch.arange(0, 4).float()\r\n",
        "y = torch.arange(0, 8, 2).float()\r\n",
        "w = torch.ones(1)\r\n",
        "         \r\n",
        "J_func(w,x,y) \r\n",
        "\r\n",
        "        \r\n",
        "    "
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n",
            "loss 14.0 , gradient -28.0 , weight tensor([1.2800])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6832fd44-4e88-41e3-bf04-c6bd973408bf"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "J_func(w, x, y)\n",
        "\n",
        "#Plotando o gráfico\n",
        "plt.rcParams['figure.figsize'] = (8,5)\n",
        "plt.plot(it,Loss,'o')\n",
        "plt.title('Iterações x Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iterações')\n",
        "plt.show()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n",
            "loss 14.0 , gradient -28.0 , weight tensor([1.2800])\n",
            "iteration 1\n",
            "loss 7.2576003074646 , gradient -20.160001754760742 , weight tensor([1.4816])\n",
            "iteration 2\n",
            "loss 3.762338399887085 , gradient -14.515199661254883 , weight tensor([1.6268])\n",
            "iteration 3\n",
            "loss 1.9503965377807617 , gradient -10.450942993164062 , weight tensor([1.7313])\n",
            "iteration 4\n",
            "loss 1.011085033416748 , gradient -7.524677276611328 , weight tensor([1.8065])\n",
            "iteration 5\n",
            "loss 0.5241461992263794 , gradient -5.41776704788208 , weight tensor([1.8607])\n",
            "iteration 6\n",
            "loss 0.2717175781726837 , gradient -3.90079402923584 , weight tensor([1.8997])\n",
            "iteration 7\n",
            "loss 0.14085862040519714 , gradient -2.8085713386535645 , weight tensor([1.9278])\n",
            "iteration 8\n",
            "loss 0.07302114367485046 , gradient -2.0221714973449707 , weight tensor([1.9480])\n",
            "iteration 9\n",
            "loss 0.03785419464111328 , gradient -1.455963373184204 , weight tensor([1.9626])\n",
            "iteration 10\n",
            "loss 0.019623562693595886 , gradient -1.0482935905456543 , weight tensor([1.9730])\n",
            "iteration 11\n",
            "loss 0.010172933340072632 , gradient -0.754772424697876 , weight tensor([1.9806])\n",
            "iteration 12\n",
            "loss 0.005273666698485613 , gradient -0.5434386730194092 , weight tensor([1.9860])\n",
            "iteration 13\n",
            "loss 0.002733835019171238 , gradient -0.39127540588378906 , weight tensor([1.9899])\n",
            "iteration 14\n",
            "loss 0.0014172064838930964 , gradient -0.28171539306640625 , weight tensor([1.9928])\n",
            "iteration 15\n",
            "loss 0.0007346798083744943 , gradient -0.2028350830078125 , weight tensor([1.9948])\n",
            "iteration 16\n",
            "loss 0.0003808624460361898 , gradient -0.1460399627685547 , weight tensor([1.9962])\n",
            "iteration 17\n",
            "loss 0.0001974297920241952 , gradient -0.1051478385925293 , weight tensor([1.9973])\n",
            "iteration 18\n",
            "loss 0.00010235930676572025 , gradient -0.07571077346801758 , weight tensor([1.9981])\n",
            "iteration 19\n",
            "loss 5.3059407946420833e-05 , gradient -0.05451202392578125 , weight tensor([1.9986])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b111a5e3-0f44-4234-ad7c-0c8b58a46dd1"
      },
      "source": [
        "from __future__ import division\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "it=[]\n",
        "Loss=[]\n",
        "\n",
        "def J_func(w, x, y):\n",
        "     \n",
        "  for i in range(iteracoes): \n",
        "\n",
        "      #Loss Function\n",
        "      y_pred = w*x \n",
        "      e = y_pred - y\n",
        "      e2 = e.pow(2) \n",
        "      J = e2.sum() \n",
        "\n",
        "      #método backward() do pytorch EXERCÍCIO 3\n",
        "      if w.grad: w.grad.zero_()\n",
        "      w.retain_grad()\n",
        "      J.backward()\n",
        "      grad=w.grad.item()\n",
        "\n",
        "      w = w - learning_rate*grad\n",
        "\n",
        "      print(\"iteration {}\".format(i))\n",
        "      print (\"loss {} , gradient {} , weight {}\".format(J,grad,w))\n",
        "\n",
        "      #Para plotar o gráfico\n",
        "      it.append(i)\n",
        "      Loss.append(J)\n",
        "\n",
        "  \n",
        "J = J_func(w, x, y)\n",
        "    \n",
        "#Plotando o gráfico\n",
        "plt.rcParams['figure.figsize'] = (8,5)\n",
        "plt.plot(it,Loss,'o')\n",
        "plt.title('Iterações x Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iterações')\n",
        "plt.show()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n",
            "loss 14.0 , gradient -28.0 , weight tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "iteration 1\n",
            "loss 7.2576003074646 , gradient -20.160001754760742 , weight tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "iteration 2\n",
            "loss 3.762338399887085 , gradient -14.51519775390625 , weight tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "iteration 3\n",
            "loss 1.9503965377807617 , gradient -10.450942993164062 , weight tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "iteration 4\n",
            "loss 1.011085033416748 , gradient -7.52467679977417 , weight tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "iteration 5\n",
            "loss 0.5241461992263794 , gradient -5.417766094207764 , weight tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "iteration 6\n",
            "loss 0.2717175781726837 , gradient -3.9007928371429443 , weight tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "iteration 7\n",
            "loss 0.14085862040519714 , gradient -2.808573007583618 , weight tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "iteration 8\n",
            "loss 0.07302114367485046 , gradient -2.0221731662750244 , weight tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "iteration 9\n",
            "loss 0.03785419464111328 , gradient -1.455965280532837 , weight tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "iteration 10\n",
            "loss 0.019623562693595886 , gradient -1.0482935905456543 , weight tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "iteration 11\n",
            "loss 0.010172933340072632 , gradient -0.7547743320465088 , weight tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "iteration 12\n",
            "loss 0.005273666698485613 , gradient -0.5434384346008301 , weight tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "iteration 13\n",
            "loss 0.002733835019171238 , gradient -0.39127326011657715 , weight tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "iteration 14\n",
            "loss 0.0014172473456710577 , gradient -0.281719446182251 , weight tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "iteration 15\n",
            "loss 0.0007347092032432556 , gradient -0.20283913612365723 , weight tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "iteration 16\n",
            "loss 0.00038088360452093184 , gradient -0.14604616165161133 , weight tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "iteration 17\n",
            "loss 0.00019744501332752407 , gradient -0.10515189170837402 , weight tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "iteration 18\n",
            "loss 0.00010235930676572025 , gradient -0.07571077346801758 , weight tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "iteration 19\n",
            "loss 5.3059407946420833e-05 , gradient -0.054509878158569336 , weight tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: O valor de $\\Delta w$ deve ser pequeno, por ser proporcional ao erro que queremos obter. Se o valor não for pequeno, nossa aproximação será imprecisa, por outro lado se o valor for muito pequeno, nosso erro de aproximação também diminuirá, porém ira exigir mais trabalho do computador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a)  Método de Diferenças finitas possui complexibilidade de **$O(N)$**, devido as subtrações e divisões da iteração não alterarem a complexibilidade original de $(x_i w - y_i)^2$\n",
        "\n",
        "b) Método de backpropagation possui complexibilidade de  **$O(N^2$)**  , devido a complexibilidade das derivadas parciais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta: O valor estimado esperado para cada predição vai ser $(k)^-1$, logo a equação de entropia cruzada pode ser resumida em : \r\n",
        "\r\n",
        " $L=-log(k^-1)$, resultando em **$L= log(k)$** .\r\n",
        "\r\n",
        " A entropia cruzada será proporcional ao logaritmo do número de classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "## Fim do notebook"
      ]
    }
  ]
}